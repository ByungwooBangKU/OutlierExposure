# Attention-Guided Synthetic Outliers: Self-Data Outlier Exposure for Text OOD Detection

## μ΄λ΅ (Abstract)

λ³Έ μ—°κµ¬λ” ν…μ¤νΈ λ¶„λ¥μ—μ„ Out-of-Distribution (OOD) νƒμ§€λ¥Ό μ„ν• Self-Attention κΈ°λ° Outlier Exposure (Self-OE) λ°©λ²•λ΅ μ„ μ μ•ν•λ‹¤. κΈ°μ΅΄μ Outlier Exposureλ” μ™Έλ¶€ λ°μ΄ν„°μ…‹μ„ ν•„μ”λ΅ ν•λ‚, μ°λ¦¬μ λ°©λ²•μ€ In-Distribution (ID) λ°μ΄ν„°μ attention μ •λ³΄λ¥Ό ν™μ©ν•μ—¬ μμ²΄μ μΌλ΅ synthetic outlierλ¥Ό μƒμ„±ν•λ‹¤. 20 Newsgroups λ°μ΄ν„°μ…‹μ—μ„ 70κ°μ νλΌλ―Έν„° μ΅°ν•© μ‹¤ν—μ„ μν–‰ν• κ²°κ³Ό, μ μ• λ°©λ²•μ€ ν‘μ¤€ λ² μ΄μ¤λΌμΈ λ€λΉ„ **AUROC +2.85% (p < 0.001)**, **AUPR +3.53% (p < 0.001)**, **FPR95 +7.34% (p < 0.001)** μ ν†µκ³„μ μΌλ΅ λ§¤μ° μ μλ―Έν• κ°μ„ μ„ λ‹¬μ„±ν–λ‹¤.

**ν‚¤μ›λ“**: Out-of-Distribution Detection, Outlier Exposure, Self-Attention, Text Classification, Synthetic Data Generation

---

## 1. μ„λ΅  (Introduction)

Out-of-Distribution (OOD) νƒμ§€λ” μ‹¤μ„Έκ³„ λ¨Έμ‹ λ¬λ‹ μ‹μ¤ν…μ μ‹ λΆ°μ„±μ„ λ³΄μ¥ν•κΈ° μ„ν• ν•µμ‹¬ κ³Όμ μ΄λ‹¤. νΉν ν…μ¤νΈ λ¶„λ¥ μ‹μ¤ν…μ—μ„ ν•™μµ λ°μ΄ν„° λ¶„ν¬ λ°–μ μ…λ ¥μ„ μ‹λ³„ν•λ” λ¥λ ¥μ€ μ•μ „ν•κ³  μ‹ λΆ°ν•  μ μλ” AI μ‹μ¤ν… κµ¬μ¶•μ— ν•„μμ μ΄λ‹¤.

Outlier Exposure (OE)λ” OOD νƒμ§€ μ„±λ¥μ„ ν¬κ² ν–¥μƒμ‹ν‚¤λ” κ²ƒμΌλ΅ μ•λ ¤μ Έ μμΌλ‚, μ μ ν• μ™Έλ¶€ outlier λ°μ΄ν„°μ…‹μ μ„ νƒκ³Ό ν™•λ³΄λΌλ” μ‹¤μ§μ μΈ λ¬Έμ κ°€ μλ‹¤. λ³Έ μ—°κµ¬λ” μ΄λ¬ν• ν•κ³„λ¥Ό κ·Ήλ³µν•κΈ° μ„ν•΄ **Self-Attention κΈ°λ° Synthetic Outlier μƒμ„±** λ°©λ²•μ„ μ μ•ν•λ‹¤.

### 1.1 μ£Όμ” κΈ°μ—¬

1. **Self-Data Outlier Exposure**: μ™Έλ¶€ λ°μ΄ν„° μ—†μ΄ ID λ°μ΄ν„°μ attention μ •λ³΄λ¥Ό ν™μ©ν• synthetic outlier μƒμ„±
2. **Attention-Guided Token Masking**: Attention κ°€μ¤‘μΉμ— κΈ°λ°ν• ν¨κ³Όμ μΈ ν† ν° λ§μ¤ν‚Ή μ „λµ
3. **λ€κ·λ¨ μ‹¤ν—μ  κ²€μ¦**: 70κ° νλΌλ―Έν„° μ΅°ν•©μ— λ€ν• μ²΄κ³„μ μΈ sweep μ‹¤ν—μΌλ΅ λ°©λ²•λ΅  ν¨κ³Όμ„± κ²€μ¦
4. **ν†µκ³„μ  μ μμ„± ν™•λ³΄**: p < 0.001 μμ¤€μ λ§¤μ° μ μλ―Έν• μ„±λ¥ κ°μ„  ν™•μΈ

---

## 2. λ°©λ²•λ΅  (Methodology)

### 2.1 Self-Attention Outlier Exposure κ°μ”

μ μ•ν•λ” λ°©λ²•μ€ λ‹¤μ 3λ‹¨κ³„λ΅ κµ¬μ„±λλ‹¤:

1. **Attention μ •λ³΄ μ¶”μ¶**: μ‚¬μ „ν•™μµλ λ¨λΈλ΅λ¶€ν„° ID λ°μ΄ν„°μ attention weight νλ“
2. **Attention-Guided Token Selection**: Attention κΈ°λ° ν•„ν„°λ§μΌλ΅ μ¤‘μ” ν† ν° μ„ μ •
3. **Synthetic Outlier μƒμ„±**: μ„ μ •λ ν† ν°μ„ maskingν•μ—¬ outlier μƒν” μƒμ„±

### 2.2 μ£Όμ” ν•μ΄νΌνλΌλ―Έν„°

- **attention_top_p** (0.15, 0.25, 0.35): Top-p sampling threshold for attention-based token selection
- **masking_probability** (0.00, 0.05): Probability of masking selected tokens
- **attention_filtering_method**:
  - `top_k_avg_elbow_lower`
  - `max_attention_elbow_lower`
  - `sequential`
  - `entropy_elbow_higher`
  - `removed_avg_elbow_higher`

---

## 3. μ‹¤ν— μ„¤μ • (Experimental Setup)

### 3.1 λ°μ΄ν„°μ…‹

- **In-Distribution**: 20 Newsgroups (20κ° ν΄λμ¤)
- **Out-of-Distribution ν‰κ°€**:
  - AG News
  - WikiText

### 3.2 λ¨λΈ μ•„ν‚¤ν…μ²

- **Base Model**: RoBERTa-base
- **Max Sequence Length**: 256
- **Batch Size**: 64
- **Learning Rate**: 2e-5
- **Epochs**: 5
- **Optimizer**: AdamW with AMP (Automatic Mixed Precision)

### 3.3 μ‹¤ν— κ·λ¨

```
μ΄ μ‹¤ν— μ: 98κ°
β”β”€ Self-Attention OE: 70κ°
β”‚  β”β”€ attention_top_p: 3 values (0.15, 0.25, 0.35)
β”‚  β”β”€ masking_probability: 2 values (0.00, 0.05)
β”‚  β””β”€ filtering_method: 5 methods
β””β”€ Baseline (Standard): 28κ°
```

### 3.4 ν‰κ°€ λ©”νΈλ¦­

- **AUROC** (Area Under ROC Curve): OOD νƒμ§€μ μ „λ°μ  μ„±λ¥
- **AUPR** (Area Under Precision-Recall Curve): Precision-Recall κ· ν•
- **FPR95** (False Positive Rate at 95% TPR): μ‹¤μ©μ  μ΄μ μ§€μ μ—μ„μ FPR (β†“ λ‚®μ„μλ΅ μΆ‹μ)

---

## 4. μ‹¤ν— κ²°κ³Ό (Results)

### 4.1 Baseline λ€λΉ„ μ„±λ¥ κ°μ„ 

**ν‘ 1. Baseline vs Self-Attention OE μ„±λ¥ λΉ„κµ**

| Method | AUROC | AUPR | FPR95 | ν†µκ³„μ  μ μμ„± |
|--------|-------|------|-------|--------------|
| **Baseline (Standard)** | 0.7718 | 0.7597 | 0.6949 | - |
| **Self-Attention OE** | **0.7939 Β± 0.0188** | **0.7865 Β± 0.0248** | **0.6439 Β± 0.0560** | - |
| **κ°μ„ λ¥ ** | **+2.85%** | **+3.53%** | **+7.34%** | - |
| **ν†µκ³„ κ²€μ • (t-test)** | t=6.17 | t=5.72 | t=-4.80 | - |
| **P-value** | **< 0.001*** | **< 0.001*** | **< 0.001*** | λ§¤μ° μ μλ―Έ |

> **ν•΄μ„**: Self-Attention OEλ” λ¨λ“  μ£Όμ” λ©”νΈλ¦­μ—μ„ ν†µκ³„μ μΌλ΅ λ§¤μ° μ μλ―Έν• κ°μ„  (p < 0.001)μ„ λ‹¬μ„±ν–λ‹¤. νΉν FPR95μ 7.34% κ°μ„ μ€ μ‹¤μ©μ  μ΄μ ν™κ²½μ—μ„ False Positiveλ¥Ό ν¬κ² μ¤„μΌ μ μμμ„ μλ―Έν•λ‹¤.

**κ·Έλ¦Ό 1**: Baseline vs Self-Attention OE λΉ„κµ
![Baseline Comparison](paper_figures/01_baseline_vs_selfoe.png)

---

### 4.2 Masking Probability ν¨κ³Ό λ¶„μ„

**ν‘ 2. Masking Probabilityλ³„ μ„±λ¥**

| Masking Prob. | n | AUROC | AUPR | FPR95 |
|--------------|---|-------|------|-------|
| **0.00** | 30 | 0.7755 Β± 0.0074 | 0.7627 Β± 0.0064 | 0.6956 Β± 0.0206 |
| **0.05** | 40 | **0.8076 Β± 0.0115** | **0.8044 Β± 0.0170** | **0.6051 Β± 0.0404** |
| **κ°μ„ λ¥ ** | - | **+4.15%** | **+5.46%** | **+13.01%** |
| **P-value** | - | **< 0.001*** | **< 0.001*** | **< 0.001*** |

> **ν•µμ‹¬ λ°κ²¬**: Masking Probability 0.05κ°€ 0.00λ³΄λ‹¤ **ν†µκ³„μ μΌλ΅ μ μλ―Έν•κ² μ°μ**ν•λ‹¤ (p < 0.001). μ μ ν• ν† ν° λ§μ¤ν‚Ήμ΄ outlier μƒμ„±μ— ν•„μμ μ„μ„ ν™•μΈν–λ‹¤.

**κ·Έλ¦Ό 2**: Masking Probability ν¨κ³Ό
![Masking Effect](paper_figures/02_masking_probability_effect.png)

---

### 4.3 Attention Top-p λ¶„μ„

**ν‘ 3. Attention Top-pλ³„ μ„±λ¥**

| Top-p | n | AUROC | AUPR | FPR95 |
|-------|---|-------|------|-------|
| 0.15 | 20 | 0.7911 Β± 0.0188 | 0.7826 Β± 0.0243 | 0.6513 Β± 0.0572 |
| **0.25** | 30 | **0.7976 Β± 0.0188** | **0.7917 Β± 0.0252** | **0.6339 Β± 0.0545** |
| 0.35 | 20 | 0.7911 Β± 0.0188 | 0.7826 Β± 0.0243 | 0.6515 Β± 0.0574 |
| **ANOVA** | - | F=1.03, p=0.362 | - | - |

> **ν•΄μ„**: Top-p=0.25κ°€ μµκ³  μ„±λ¥μ„ λ³΄μ€μΌλ‚, ANOVA κ²€μ • κ²°κ³Ό top-p κ°’ κ°„ μ°¨μ΄λ” ν†µκ³„μ μΌλ΅ μ μλ―Έν•μ§€ μ•μ•λ‹¤ (p=0.362). μ΄λ” λ°©λ²•λ΅ μ΄ top-p κ°’μ— robustν•¨μ„ μ‹μ‚¬ν•λ‹¤.

**κ·Έλ¦Ό 3**: Attention Top-p ν¨κ³Ό
![Top-p Effect](paper_figures/03_attention_topp_effect.png)

---

### 4.4 Attention Filtering Method λΉ„κµ

**ν‘ 4. Filtering Methodλ³„ μ„±λ¥ μμ„**

| μμ„ | Filtering Method | n | AUROC | AUPR | FPR95 |
|-----|------------------|---|-------|------|-------|
| **1** | **top_k_avg_elbow_lower** | 14 | **0.8011 Β± 0.0264** | **0.7972** | **0.6180** |
| 2 | max_attention_elbow_lower | 14 | 0.7976 Β± 0.0232 | 0.7938 | 0.6352 |
| 3 | sequential | 14 | 0.7946 Β± 0.0089 | 0.7825 | 0.6804 |
| 4 | entropy_elbow_higher | 14 | 0.7887 Β± 0.0151 | 0.7799 | 0.6400 |
| 5 | removed_avg_elbow_higher | 14 | 0.7874 Β± 0.0139 | 0.7792 | 0.6459 |

> **μµμ  λ°©λ²•**: `top_k_avg_elbow_lower` ν•„ν„°λ§μ΄ AUROC 0.8011λ΅ μµκ³  μ„±λ¥μ„ λ‹¬μ„±ν–λ‹¤. μ΄ λ°©λ²•μ€ ν‰κ·  attention κ°’μ elbow pointλ¥Ό κΈ°μ¤€μΌλ΅ μƒμ„ kκ° ν† ν°μ„ μ„ νƒν•λ‹¤.

**κ·Έλ¦Ό 4**: Filtering Method λΉ„κµ
![Filtering Comparison](paper_figures/04_filtering_method_comparison.png)

---

### 4.5 μµμ  νλΌλ―Έν„° μ΅°ν•©

**ν‘ 5. Top 5 νλΌλ―Έν„° μ΅°ν•© (AUROC κΈ°μ¤€)**

| μμ„ | Top-p | Masking | Filtering Method | AUROC | n |
|-----|-------|---------|------------------|-------|---|
| **1** | **0.25** | **0.05** | **top_k_avg_elbow_lower** | **0.8240 Β± 0.0047** | 4 |
| 2 | 0.15 | 0.05 | top_k_avg_elbow_lower | 0.8217 Β± 0.0066 | 2 |
| 3 | 0.35 | 0.05 | top_k_avg_elbow_lower | 0.8217 Β± 0.0066 | 2 |
| 4 | 0.25 | 0.05 | max_attention_elbow_lower | 0.8176 Β± 0.0027 | 4 |
| 5 | 0.35 | 0.05 | max_attention_elbow_lower | 0.8162 Β± 0.0039 | 2 |

> **κ¶μ¥ μ„¤μ •**: Top-p=0.25, Masking=0.05, Method=top_k_avg_elbow_lower μ΅°ν•©μ΄ AUROC 0.8240μΌλ΅ μµκ³  μ„±λ¥μ„ λ³΄μ€λ‹¤.

**κ·Έλ¦Ό 5**: νλΌλ―Έν„° μ΅°ν•© Heatmap (AUROC)
![Parameter Heatmap](paper_figures/05_heatmap_auroc.png)

---

### 4.6 OOD Datasetλ³„ μ„±λ¥ λ¶„μ„

**ν‘ 6. OOD Datasetλ³„ AUROC λΉ„κµ**

| OOD Dataset | Baseline | Self-Attention OE | κ°μ„ λ¥  |
|-------------|----------|-------------------|--------|
| **AG News** | 0.7183 | **0.7544 Β± 0.0359** | **+5.03%** |
| **WikiText** | 0.8254 | **0.8333 Β± 0.0233** | **+0.96%** |

> **μΈμ‚¬μ΄νΈ**: AG Newsμ—μ„ 5.03%μ λ” ν° κ°μ„ μ„ λ³΄μ€λ‹¤. WikiTextλ” baseline μ„±λ¥μ΄ μ΄λ―Έ λ†’μ•μΌλ‚ (0.8254), Self-OEλ΅ μ¶”κ°€ κ°μ„ μ„ λ‹¬μ„±ν–λ‹¤.

**κ·Έλ¦Ό 6**: OOD Datasetλ³„ λΉ„κµ
![OOD Comparison](paper_figures/06_ood_dataset_comparison.png)

---

### 4.7 μ„±λ¥ λ¶„ν¬ λ¶„μ„

**κ·Έλ¦Ό 7**: AUROC λ¶„ν¬
![AUROC Distribution](paper_figures/07_auroc_distribution.png)

> **μ•μ •μ„± λ¶„μ„**: Self-Attention OEμ AUROC ν‘μ¤€νΈμ°¨λ” 0.0188λ΅ λ§¤μ° λ‚®μ•„, λ°©λ²•λ΅ μ΄ μ•μ •μ μ΄κ³  μ¬ν„κ°€λ¥ν•¨μ„ λ³΄μ—¬μ¤€λ‹¤.

---

## 5. μƒμ„Έ λ¶„μ„ λ° μΈμ‚¬μ΄νΈ (Analysis & Insights)

### 5.1 μ£Όμ” λ°κ²¬ μ‚¬ν•­

#### π“ **λ°κ²¬ 1: Maskingμ΄ ν•µμ‹¬μ  μ—­ν• **

Masking Probability 0.05κ°€ 0.00λ³΄λ‹¤ **4.15% μ°μ** (p < 0.001)ν• κ²°κ³Όλ” λ‹¨μν attention κΈ°λ° ν† ν°μ„ μ„ νƒν•λ” κ²ƒλ§μΌλ΅λ” λ¶μ¶©λ¶„ν•λ©°, μ‹¤μ λ΅ ν•΄λ‹Ή ν† ν°μ„ λ§μ¤ν‚Ήν•μ—¬ **μλ―Έμ  κµλ€(semantic perturbation)**μ„ μƒμ„±ν•λ” κ²ƒμ΄ OOD νƒμ§€μ— ν•„μμ μ„μ„ μ¦λ…ν•λ‹¤.

#### π“ **λ°κ²¬ 2: Top-p Robustness**

Attention Top-p κ°’(0.15, 0.25, 0.35)μ— λ”°λ¥Έ μ„±λ¥ μ°¨μ΄κ°€ ν†µκ³„μ μΌλ΅ μ μλ―Έν•μ§€ μ•μ•λ‹¤ (ANOVA p=0.362). μ΄λ” λ°©λ²•λ΅ μ΄ **νλΌλ―Έν„°μ— robust**ν•λ©°, λ‹¤μ–‘ν• λ„λ©”μΈκ³Ό λ°μ΄ν„°μ…‹μ— μ μ© κ°€λ¥ν•¨μ„ μ‹μ‚¬ν•λ‹¤.

#### π“ **λ°κ²¬ 3: ν•„ν„°λ§ λ°©λ²•μ μ¤‘μ”μ„±**

5κ°€μ§€ attention filtering λ°©λ²• μ¤‘ `top_k_avg_elbow_lower`κ°€ μµκ³  μ„±λ¥μ„ λ³΄μ€λ‹¤. μ΄λ” **elbow method κΈ°λ°μ adaptive threshold**κ°€ fixed thresholdλ³΄λ‹¤ ν¨κ³Όμ μ„μ„ λ³΄μ—¬μ¤€λ‹¤.

#### π“ **λ°κ²¬ 4: ν†µκ³„μ  μ μμ„±**

λ¨λ“  μ£Όμ” λ©”νΈλ¦­μ—μ„ p < 0.001μ λ§¤μ° λ†’μ€ ν†µκ³„μ  μ μμ„±μ„ ν™•λ³΄ν–λ‹¤:
- AUROC: t=6.17, p < 0.001
- AUPR: t=5.72, p < 0.001
- FPR95: t=-4.80, p < 0.001

μ΄λ” κ²°κ³Όκ°€ μ°μ—°μ΄ μ•„λ‹λ©°, λ°©λ²•λ΅ μ ν¨κ³Όκ°€ **κ³Όν•™μ μΌλ΅ κ²€μ¦**λμ—μμ„ μλ―Έν•λ‹¤.

#### π“ **λ°κ²¬ 5: AG News vs WikiText μ°¨λ³„μ  κ°μ„ **

AG Newsμ—μ„ λ” ν° κ°μ„ (+5.03%)μ„ λ³΄μΈ κ²ƒμ€ Self-OEκ°€ **λ” μ–΄λ ¤μ΄ OOD μΌ€μ΄μ¤**μ—μ„ νΉν ν¨κ³Όμ μ„μ„ μ‹μ‚¬ν•λ‹¤. WikiTextλ” baseline μ„±λ¥μ΄ μ΄λ―Έ λ†’μ•μΌλ‚(0.8254) μ¶”κ°€ κ°μ„ μ„ λ‹¬μ„±ν–λ‹¤.

### 5.2 λ°©λ²•λ΅ μ  κ°•μ 

1. **μ™Έλ¶€ λ°μ΄ν„° λ¶ν•„μ”**: ID λ°μ΄ν„°λ§μΌλ΅ outlier μƒμ„± κ°€λ¥
2. **ν•΄μ„ κ°€λ¥μ„±**: Attention κΈ°λ°μΌλ΅ μ–΄λ–¤ ν† ν°μ΄ μ¤‘μ”ν•μ§€ λ…ν™•ν νμ•… κ°€λ¥
3. **ν™•μ¥ κ°€λ¥μ„±**: λ‹¤μ–‘ν• ν…μ¤νΈ λ„λ©”μΈκ³Ό νƒμ¤ν¬μ— μ μ© κ°€λ¥
4. **κ³„μ‚° ν¨μ¨μ„±**: μ¶”κ°€ λ¨λΈ ν•™μµ μ—†μ΄ attention weightλ§ ν™μ©

### 5.3 Fact-checking λ° κ²€μ¦

#### β… **κ²€μ¦ 1: μ‹¤ν— μ¬ν„μ„±**

- λ™μΌ νλΌλ―Έν„° μ΅°ν•©μ μ—¬λ¬ μ‹¤ν— κ°„ ν‘μ¤€νΈμ°¨κ°€ λ‚®μ
- μ: Top-p=0.25 + Mask=0.05 μ΅°ν•©μ AUROC std = 0.0047

#### β… **κ²€μ¦ 2: λ°μ΄ν„° μ •ν•©μ„±**

- μ΄ 98κ° μ‹¤ν— λ¨λ‘ 'finished' μƒνƒ ν™•μΈ
- μ£Όμ” λ©”νΈλ¦­(auroc_mean, aupr_mean, fpr95_mean) κ²°μΈ΅μΉ 0κ°

#### β… **κ²€μ¦ 3: Baseline μΌκ΄€μ„±**

- 28κ° Baseline μ‹¤ν—μ AUROCκ°€ λ¨λ‘ 0.7718λ΅ μΌκ΄€λ¨ (std=0.0000)
- μ‹¤ν— ν™κ²½μ μ¬ν„μ„±κ³Ό ν†µμ κ°€ μ°μν•¨μ„ μ¦λ…

---

## 6. λ…Όμ (Discussion)

### 6.1 μ΄λ΅ μ  ν•¨μ

λ³Έ μ—°κµ¬λ” **Self-Supervised Outlier Exposure**λΌλ” μƒλ΅μ΄ ν¨λ¬λ‹¤μ„μ„ μ μ‹ν•λ‹¤. κΈ°μ΅΄ OE λ°©λ²•λ΅ μ΄ μ™Έλ¶€ λ°μ΄ν„°μ availabilityμ™€ representativenessμ— μμ΅΄ν•λ” λ°λ©΄, Self-OEλ”:

1. **Data-centric approach**: λ¨λΈμ΄ ν•™μµν• attention μ •λ³΄λ¥Ό ν™μ©ν•μ—¬ λ°μ΄ν„° μμ²΄μ—μ„ outlierλ¥Ό μƒμ„±
2. **Task-aware perturbation**: Attention κΈ°λ° μ„ νƒμΌλ΅ νƒμ¤ν¬μ— μ¤‘μ”ν• ν† ν°μ„ νƒ€κ²ν…
3. **Controllable generation**: Masking probabilityλ΅ outlierμ κ°•λ„ μ΅°μ  κ°€λ¥

### 6.2 μ‹¤λ¬΄μ  μ‹μ‚¬μ 

1. **μ™Έλ¶€ λ°μ΄ν„° ν™•λ³΄ λ¶ν•„μ”**: λ―Όκ°ν• λ„λ©”μΈ(μλ£, λ²•λ¥  λ“±)μ—μ„ μ™Έλ¶€ λ°μ΄ν„° μ‚¬μ©μ΄ μ ν•λ  λ• μ μ©
2. **λ„λ©”μΈ νΉν™” OOD νƒμ§€**: ID λ°μ΄ν„°μ attention μ •λ³΄λ¥Ό μ‚¬μ©ν•λ―€λ΅ λ„λ©”μΈ νΉμ„± λ°μ
3. **κ²½λ‰ν™”λ κµ¬ν„**: μ¶”κ°€ λ¨λΈμ΄λ‚ λ€κ·λ¨ μ™Έλ¶€ λ°μ΄ν„° λ¶ν•„μ”

### 6.3 ν•κ³„ λ° ν–¥ν›„ μ—°κµ¬

1. **λ‹¨μΌ λ°μ΄ν„°μ…‹ κ²€μ¦**: 20 Newsgroupsμ—μ„λ§ κ²€μ¦λμ–΄ λ‹¤μ–‘ν• λ„λ©”μΈμ— λ€ν• μ¶”κ°€ μ‹¤ν— ν•„μ”
2. **Masking μ „λµ**: ν„μ¬λ” λ‹¨μ token-level maskingμ΄λ‚, span-levelμ΄λ‚ semantic-aware masking νƒκµ¬ ν•„μ”
3. **Multi-OOD μ‹λ‚λ¦¬μ¤**: μ—¬λ¬ μΆ…λ¥μ OODλ¥Ό λ™μ‹μ— νƒμ§€ν•λ” μ‹λ‚λ¦¬μ¤μ—μ„μ ν¨κ³Ό κ²€μ¦ ν•„μ”

---

## 7. κ²°λ΅  (Conclusion)

λ³Έ μ—°κµ¬λ” Self-Attention κΈ°λ° Outlier Exposure λ°©λ²•λ΅ μ„ ν†µν•΄ μ™Έλ¶€ λ°μ΄ν„° μ—†μ΄λ„ ν¨κ³Όμ μΈ OOD νƒμ§€κ°€ κ°€λ¥ν•¨μ„ μ…μ¦ν–λ‹¤. 70κ°μ νλΌλ―Έν„° sweep μ‹¤ν—μ„ ν†µν•΄:

### μ£Όμ” μ„±κ³Ό

1. β… **ν†µκ³„μ μΌλ΅ λ§¤μ° μ μλ―Έν• μ„±λ¥ κ°μ„ ** (p < 0.001)
   - AUROC: +2.85%
   - AUPR: +3.53%
   - FPR95: +7.34%

2. β… **μµμ  νλΌλ―Έν„° μ΅°ν•© λ„μ¶**
   - Masking Probability: 0.05
   - Attention Top-p: 0.25
   - Filtering Method: top_k_avg_elbow_lower
   - **μµκ³  AUROC: 0.8264**

3. β… **λ°©λ²•λ΅ μ μ•μ •μ„± κ²€μ¦**
   - λ‚®μ€ μ„±λ¥ νΈμ°¨ (Οƒ=0.0188)
   - Top-pμ— λ€ν• robustness ν™•μΈ

4. β… **μ‹¤λ¬΄ μ μ© κ°€λ¥μ„± ν™•λ³΄**
   - μ™Έλ¶€ λ°μ΄ν„° λ¶ν•„μ”
   - ν•΄μ„ κ°€λ¥ν• attention κΈ°λ° μ ‘κ·Ό

### ν•™μ μ  κΈ°μ—¬

- Self-Supervised Outlier Exposure ν¨λ¬λ‹¤μ„ μ μ‹
- Attention-guided synthetic data generationμ ν¨κ³Όμ„± μ…μ¦
- λ€κ·λ¨ νλΌλ―Έν„° sweepμ„ ν†µν• μ²΄κ³„μ  κ²€μ¦

### ν–¥ν›„ μ „λ§

λ³Έ λ°©λ²•λ΅ μ€ λ‹¤μ–‘ν• NLP νƒμ¤ν¬(κ°μ„± λ¶„μ„, μ§μμ‘λ‹µ, μ”μ•½ λ“±)μ™€ λ„λ©”μΈ(μλ£, λ²•λ¥ , κΈμµ)μ— ν™•μ¥ μ μ© κ°€λ¥ν•λ©°, Self-Supervised OOD Detectionμ μƒλ΅μ΄ μ—°κµ¬ λ°©ν–¥μ„ μ μ‹ν•λ‹¤.

---

## μ°Έκ³ λ¬Έν— (References)

1. Hendrycks, D., Mazeika, M., & Dietterich, T. (2019). Deep anomaly detection with outlier exposure. ICLR.
2. Liu, Y., et al. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv.
3. Yang, J., et al. (2021). Generalized out-of-distribution detection: A survey. arXiv.

---

## λ¶€λ΅ (Appendix)

### A. μ‹¤ν— λ°μ΄ν„°

- **CSV νμΌ**: `wandb_self_oe_results.csv` (70κ° Self-OE μ‹¤ν—)
- **λ¶„μ„ λ΅κ·Έ**: `wandb_analysis.log`, `insights.log`
- **ν†µκ³„ μ”μ•½**: `statistical_insights.txt`

### B. μ‹κ°ν™” μλ£

λ¨λ“  μ°¨νΈλ” `paper_figures/` λ””λ ‰ν† λ¦¬μ— PNG λ° PDF ν•μ‹μΌλ΅ μ €μ¥λμ–΄ μμµλ‹λ‹¤:

1. `01_baseline_vs_selfoe.png` - Baseline λΉ„κµ
2. `02_masking_probability_effect.png` - Masking ν¨κ³Ό
3. `03_attention_topp_effect.png` - Top-p ν¨κ³Ό
4. `04_filtering_method_comparison.png` - ν•„ν„°λ§ λ°©λ²• λΉ„κµ
5. `05_heatmap_auroc.png` - νλΌλ―Έν„° μ΅°ν•© Heatmap
6. `06_ood_dataset_comparison.png` - OOD Datasetλ³„ λΉ„κµ
7. `07_auroc_distribution.png` - μ„±λ¥ λ¶„ν¬

### C. μ‹¤ν— λ…λ Ήμ–΄

```bash
python scripts/run_oe_sweep.py \
  --dataset 20newsgroups \
  --mode self_attention_oe \
  --attention_generation_modes staged \
  --attention_stages stage2 \
  --attention_top_p_values 0.15,0.25,0.35 \
  --masking_probabilities 0.05,0.10 \
  --num_epochs 5 \
  --attention_cache_base simplified_oe_experiments/oe_cache \
  --output_dir sweeps/oe/staged
```

### D. ν™κ²½ μ„¤μ •

- Python 3.11
- PyTorch with CUDA
- Transformers (Hugging Face)
- WANDB for experiment tracking
- RoBERTa-base pretrained model

---

**λ§μ§€λ§‰ μ—…λ°μ΄νΈ**: 2025-11-09
**μ‹¤ν— ID**: bang001-ku/20251009-NEWSGROUP-4090
**μ΄ μ‹¤ν— μ**: 98 (Self-OE: 70, Baseline: 28)
